# -*- coding: utf-8 -*-
"""Risk_Env_Projet_LLM_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WdvltM3IW7cxYI9AbkS7NK2SAG_gorIm
"""

import os
import sys
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Commented out IPython magic to ensure Python compatibility.
# Installing Required Libraries
# %pip install python-docx
# %pip install python-pptx
# %pip install PyPDF2
# %pip install langchain
# %pip install langchain_community
# %pip install langchain_google_genai
# %pip install langchain_text_splitters
# %pip install sentence-transformers
# %pip install faiss-cpu
# %pip install cohere

# necessary Imports
import os
import sys
import pickle

# Get the paths from environment variables
ASSETS_PATH = os.environ.get('ASSETS_PATH', os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'assets'))
DOCS_PATH = os.environ.get('DOCS_PATH', os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'docs'))
ROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))

# from docx import Document
# from pptx import Presentation
from PyPDF2 import PdfReader
from langchain_community.llms import Cohere
# from langchain_cohere import Cohere  # ‚úÖ Nouvelle importation correcte
from langchain_community.vectorstores import FAISS
# from langchain_google_genai import GoogleGenerativeAI
# from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain_google_genai import GoogleGenerativeAIEmbeddings
# from langchain.embeddings import HuggingFaceEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings  # ‚úÖ Correct
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import AIMessage, HumanMessage
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.prompts  import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder


"""<a id = '1.1'></a>
<p style = "font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; "><strong>Data Loading</strong></p>
<p style = "font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' ">For This Notebook, I have taken Two PDF Files about Tunisan Environment Risks. You can add more different types of data in the code.</p>
<ul>
    <li style = "font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; "><strong>1st PDF file:-</strong> CLIMATE RISK COUNTRY PROFILE TUNISIA. </li>
    <li style = "font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; "><strong>2nd PDF file:-</strong> TUNISIA: Environment and sustainable development issues and policies.</li>
</ul>
"""

# D√©finition des chemins d'acc√®s aux fichiers PDF
pdf_paths = [
    os.path.join(DOCS_PATH, "15727-WB_Tunisia Country Profile-WEB.pdf"),
    os.path.join(DOCS_PATH, "Environment and sustainable Tunisia.pdf"),
    os.path.join(DOCS_PATH, "Maghreb-Technical-Note-11.pdf")
]

# Chemin pour sauvegarder l'entra√Ænement
faiss_index_path = os.path.join(ROOT_DIR, "faiss_index.pkl")

# Fonction pour extraire le texte d'un PDF
def extract_text_from_pdf(pdf_path):
    text = ""
    with open(pdf_path, 'rb') as pdf_file:
        pdf_reader = PdfReader(pdf_file)
        for page in pdf_reader.pages:
            page_text = page.extract_text()
            if page_text:
                text += page_text + "\n"
    return text

"""<a id = '1.3'></a>
<p style = "font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; "><strong>Chunking</strong></p>
<p style = "font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' "> In this step I am creating the chunks of data, for this step I am using Recursive Character Splitter which break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won‚Äôt fit in a model‚Äôs finite context window.</p>

<a id = '1.4'></a>
<p style = "font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; "><strong>Embeddings Creation</strong></p>

<p style = "font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' ">Embeddings creation is a crucial preprocessing step in the development of document-based Question and Answering (Q&A) systems. This process involves converting textual data from documents and questions into dense, high-dimensional vectors known as embeddings. These embeddings are designed to capture the semantic meaning of words, sentences, or even entire documents, enabling the Q&A system to understand and process natural language more effectively.</p>

<a id = '1.5'></a>
<p style = "font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; "><strong>Indexing</strong></p>
<p style = "font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' ">Indexing data using Facebook AI Similarity Search (FAISS) is a pivotal step in developing efficient and scalable document-based Question and Answering (Q&A) systems. FAISS is a library that facilitates the efficient search for similarities in large datasets, especially useful for tasks involving high-dimensional vectors like text embeddings. When applied to document-based Q&A, FAISS indexes the embeddings of document chunks (e.g., paragraphs, sentences) to optimize the retrieval process.</p>
"""

# Configuration des cl√©s API (remplacez par vos propres cl√©s)

# Get API tokens from environment variables
os.environ['HuggingFaceHub_API_Token'] = os.getenv('HUGGINGFACE_API_TOKEN', 'YOUR_HUGGINGFACE_API_TOKEN')
# os.environ['GOOGLE_API_KEY'] = os.getenv('GOOGLE_API_KEY', 'YOUR_GOOGLE_API_KEY')
os.environ['cohere_api_key'] = os.getenv('COHERE_API_KEY', 'YOUR_COHERE_API_KEY')

# V√©rification si l'index FAISS est d√©j√† sauvegard√©
if os.path.exists(faiss_index_path):
    print(f"Loading saved FAISS index from {faiss_index_path}...")
    with open(faiss_index_path, "rb") as f:
        vectorstore = pickle.load(f)
else:
    # Extraction et fusion des textes
    all_text = "\n".join([extract_text_from_pdf(pdf) for pdf in pdf_paths])
    print(f"Total text length: {len(all_text)} characters.")

    # D√©coupage du texte en morceaux pour l'indexation
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len,
        separators=['\n', '\n\n', ' ', '']
    )

    chunks = text_splitter.split_text(text=all_text)
    print(f"Total chunks created: {len(chunks)}")

    # Get API tokens from environment variables
    os.environ['HuggingFaceHub_API_Token'] = os.getenv('HUGGINGFACE_API_TOKEN', 'YOUR_HUGGINGFACE_API_TOKEN')
    os.environ['cohere_api_key'] = os.getenv('COHERE_API_KEY', 'YOUR_COHERE_API_KEY')

    # Initialisation du mod√®le d'embeddings
    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')

    # Indexation des donn√©es avec FAISS
    vectorstore = FAISS.from_texts(chunks, embedding=embeddings)

    # Sauvegarde de l'index pour une utilisation future
    with open(faiss_index_path, "wb") as f:
        pickle.dump(vectorstore, f)
    print("FAISS index saved successfully.")

"""<a id = '2.1'></a>
<p style = "font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; "><strong>Retriever</strong></p>
<p style = "font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' ">In the development of document-based Question and Answering (Q&A) systems, creating a retriever is a crucial step that directly impacts the system's ability to find relevant information efficiently. The retriever utilizes the pre-indexed embeddings of document chunks, searching through them to find the most relevant pieces of content in response to a user query. This process involves setting up a retrieval mechanism that leverages similarity search to identify the best matches for the query embeddings within the indexed data.</p>
"""

# Cr√©ation du retriever pour la recherche de contexte
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 6})

"""<a id = '2.2'></a>
<p style = "font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; "><strong>LLM Models</strong></p>

<ul>
    <li style = "font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; ">Large Language Models (LLMs) are advanced artificial intelligence systems designed to understand, generate, and interact with human language in a way that mimics human-like understanding. These models are trained on vast amounts of text data, allowing them to grasp the nuances of language, including grammar, context, and even cultural references. The capabilities of LLMs extend beyond simple text generation; they can perform a variety of tasks such as translation, summarization, question answering, and even code generation.</li>
    <li style = "font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS'; ">One of the key technologies behind LLMs is the Transformer architecture, which enables the model to pay attention to different parts of the input text differently, thereby understanding the context and relationships between words and phrases more effectively. This architecture has led to significant improvements in natural language processing tasks and is the foundation of many state-of-the-art LLMs.</li>
</ul>
"""

# D√©finition du prompt en anglais
prompt_template = """Answer the question in a way that a child (5-18 years old) can easily understand.
Use simple words and short sentences. Imagine you are explaining this to a young friend.
If the answer is not found in the context, say "Answer not available in context".

Context:
{context}

Question:
{question}

Answer:"""

prompt = PromptTemplate.from_template(template=prompt_template)

# Fonction pour formater les documents r√©cup√©r√©s par FAISS
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# Fonction pour g√©n√©rer des r√©ponses ind√©pendantes
def generate_answer(question):
    # R√©initialisation du mod√®le pour chaque question
    cohere_llm = Cohere(model="command", temperature=0.1, cohere_api_key=os.getenv('cohere_api_key'))

    rag_chain = (
        {"context": retriever | format_docs, "question": RunnablePassthrough()}
        | prompt
        | cohere_llm
        | StrOutputParser()
    )

    return rag_chain.invoke(question)

# üîπ Test du mod√®le avec des questions ind√©pendantes
questions = [
    "What are the main environmental risks in Tunisia?",
    "How is Tunisia addressing climate change?",
    "What are the main types of trash that exists on the seaside in Tunisia?"
]

"""<a id = '3.0'></a>
<p style = "font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; "><strong>Results</strong></p>
"""

# # G√©n√©rer et afficher les r√©ponses
# for i, question in enumerate(questions, 1):
#     answer = generate_answer(question)
#     print(f"Q{i}: {question}\nA{i}: {answer}\n")

"""<a id = '4.0'></a>
<p style = "font-size : 20px; color : #34656d ; font-family : 'Comic Sans MS'; "><strong>Conclusion</strong></p>
<p style = "font-size : 15px; color : #810000 ; font-family : 'Comic Sans MS' ">In conclusion, this Kaggle notebook has successfully demonstrated the application of Retrieval-Augmented Generation (RAG) for multi-document Question and Answering. It showcased the power of combining retrieval and generation capabilities to provide accurate, context-aware answers sourced from multiple documents. Through detailed examples, performance evaluations, and interactive demonstrations, the notebook highlights the efficiency and scalability of RAG in handling complex Q&A tasks.</p>

<p style = "font-size : 13px; color : #810000 ; font-family : 'Comic Sans MS' ">
If you found this helpful an upvote would be very much appreciated :-)</p>
"""